#!/usr/bin/env python3
"""
True Source Benchmark - Tests how having source text affects translation accuracy
"""

import argparse
import os
import random
import json
from pathlib import Path
from tqdm import tqdm
from statistics import mean, stdev
from dotenv import load_dotenv
import litellm

# Add project root to path for imports
import sys
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from metrics import chrF_plus, normalized_edit_distance
from benchmarks.benchmark_utils import extract_xml_content, format_xml_prompt


class TrueSourceBenchmark:
    def __init__(self, corpus_dir, source_file, models=None):
        self.models = models if isinstance(models, list) else [models] if models else ["gpt-4o"]
        self.corpus_dir = Path(corpus_dir)
        self.source_file = self.corpus_dir / source_file
        
        self.target_files = [f for f in self.corpus_dir.glob('*.txt') if f != self.source_file]

    def load_file_pair(self, target_file):
        with open(self.source_file, 'r', encoding='utf-8') as f:
            source_lines = f.readlines()
        with open(target_file, 'r', encoding='utf-8') as f:
            target_lines = f.readlines()
        return source_lines, target_lines

    def get_test_cases_per_language(self, num_tests_per_language):
        """Get test cases for each target language"""
        all_language_tests = {}
        
        for target_file in self.target_files:
            try:
                source_lines, target_lines = self.load_file_pair(target_file)
                
                valid_indices = [
                    i for i in range(min(len(source_lines), len(target_lines)))
                    if len(source_lines[i].strip()) > 10 and len(target_lines[i].strip()) > 10
                ]
                
                # Need more indices to get both examples and test cases
                total_needed = num_tests_per_language + 6  # 3 for examples, rest for tests
                if len(valid_indices) < total_needed:
                    print(f"⚠️  Skipping {target_file.name} - only {len(valid_indices)} valid lines (need {total_needed})")
                    continue
                
                selected_indices = random.sample(valid_indices, total_needed)
                
                # First 3 pairs are ICL examples, rest are test cases
                example_pairs = [(source_lines[i].strip(), target_lines[i].strip()) 
                                for i in selected_indices[:3]]
                test_cases = [(source_lines[i].strip(), target_lines[i].strip()) 
                             for i in selected_indices[3:3+num_tests_per_language]]
                
                all_language_tests[target_file.name] = {
                    'example_pairs': example_pairs,
                    'test_cases': test_cases,
                    'target_file': target_file
                }
                
            except Exception as e:
                print(f"⚠️  Error loading {target_file.name}: {e}")
                continue
        
        return all_language_tests

    def translate_with_source_examples(self, test_source, example_pairs, target_lang, model):
        # Build prompt with source-target example pairs
        base_prompt = f"Translate from source to {target_lang}. Examples:\n\n"
        for src, tgt in example_pairs:
            base_prompt += f"Source: {src}\nTarget: {tgt}\n\n"
        base_prompt += f"Now translate:\nSource: {test_source}"
        
        prompt = format_xml_prompt(base_prompt, "translation", "your translation here")
        
        response = litellm.completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.1
        )
        return extract_xml_content(response.choices[0].message.content.strip(), "translation")

    def translate_with_target_only_examples(self, test_source, example_pairs, target_lang, model):
        # Build prompt with only target examples (no source)
        base_prompt = f"Translate to {target_lang}. Example target language texts:\n\n"
        for _, tgt in example_pairs:
            base_prompt += f"{tgt}\n"
        base_prompt += f"\nNow translate this to {target_lang}:\n{test_source}"
        
        prompt = format_xml_prompt(base_prompt, "translation", "your translation here")
        
        response = litellm.completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.1
        )
        return extract_xml_content(response.choices[0].message.content.strip(), "translation")

    def evaluate_translation(self, hypothesis, reference):
        return {
            'chrf': chrF_plus(hypothesis, reference),
            'edit': 1.0 - normalized_edit_distance(hypothesis, reference)
        }

    def run_benchmark(self, num_tests=15, output_file=None):
        # Get test cases for all languages
        all_language_tests = self.get_test_cases_per_language(num_tests)
        
        if not all_language_tests:
            print("❌ No valid target languages found!")
            return None
        
        print(f"🎯 True Source Benchmark")
        print(f"Models: {', '.join(self.models)}")
        print(f"Target languages: {len(all_language_tests)} languages")
        print(f"Tests per language: {num_tests}")
        print(f"ICL Examples per language: 3")
        print(f"Languages: {', '.join(all_language_tests.keys())}")
        print()
        
        # Store results for each model
        all_results = {}
        detailed_results = {}
        
        for model in self.models:
            print(f"\n🤖 Testing model: {model}")
            
            # Results structure: {language: {condition: [scores]}}
            results_by_language = {}
            model_detailed_results = []
        
            total_tests = len(all_language_tests) * num_tests * 2  # 2 conditions per test
            
            with tqdm(total=total_tests, desc=f"Testing {model}") as pbar:
                for lang_name, lang_data in all_language_tests.items():
                    target_lang = lang_name.split('-')[0] if '-' in lang_name else "target language"
                    example_pairs = lang_data['example_pairs']
                    test_cases = lang_data['test_cases']
                    
                    # Initialize results for this language
                    results_by_language[lang_name] = {"with_source": [], "without_source": []}
                    
                    for source_text, reference in test_cases:
                        # Test 1: With source-target example pairs
                        trans_with_source = self.translate_with_source_examples(source_text, example_pairs, target_lang, model)
                        scores_with = self.evaluate_translation(trans_with_source, reference)
                        
                        # Test 2: With target-only examples  
                        trans_without_source = self.translate_with_target_only_examples(source_text, example_pairs, target_lang, model)
                        scores_without = self.evaluate_translation(trans_without_source, reference)
                        
                        results_by_language[lang_name]["with_source"].append(scores_with)
                        results_by_language[lang_name]["without_source"].append(scores_without)
                        
                        model_detailed_results.append({
                        "language": lang_name,
                        "source": source_text,
                        "reference": reference,
                        "translations": {
                            "with_source": trans_with_source,
                            "without_source": trans_without_source
                        },
                        "scores": {
                            "with_source": scores_with,
                            "without_source": scores_without
                        }
                        })
                        
                        pbar.update(2)  # 2 conditions tested
            
            # Store results for this model
            all_results[model] = results_by_language
            detailed_results[model] = model_detailed_results
        
        # Print results for all models
        self.print_results(all_results)
        
        # Create aggregated summary statistics for all models
        all_summary_stats = {}
        for model, results_by_language in all_results.items():
            all_summary_stats[model] = self.compute_summary_stats(results_by_language)
        
        output_data = {
            "benchmark": "true_source",
            "models": self.models,
            "languages_tested": list(all_language_tests.keys()),
            "summary": all_summary_stats,
            "detailed_results": detailed_results
        }
        
        if output_file:
            self.save_results(all_results, detailed_results, output_file)
        
        return output_data

    def compute_summary_stats(self, results_by_language):
        """Compute both per-language and overall aggregated statistics"""
        summary_stats = {
            "per_language": {},
            "overall": {}
        }
        
        conditions = ["with_source", "without_source"]
        
        # Per-language stats
        for lang_name, lang_results in results_by_language.items():
            summary_stats["per_language"][lang_name] = {}
            for condition in conditions:
                chrf_scores = [r["chrf"] for r in lang_results[condition]]
                edit_scores = [r["edit"] for r in lang_results[condition]]
                
                summary_stats["per_language"][lang_name][condition] = {
                    "chrf_mean": mean(chrf_scores),
                    "chrf_std": stdev(chrf_scores) if len(chrf_scores) > 1 else 0.0,
                    "edit_mean": mean(edit_scores),
                    "edit_std": stdev(edit_scores) if len(edit_scores) > 1 else 0.0,
                    "num_tests": len(chrf_scores)
                }
        
        # Overall aggregated stats (combining all languages)
        for condition in conditions:
            all_chrf_scores = []
            all_edit_scores = []
            
            for lang_results in results_by_language.values():
                all_chrf_scores.extend([r["chrf"] for r in lang_results[condition]])
                all_edit_scores.extend([r["edit"] for r in lang_results[condition]])
            
            summary_stats["overall"][condition] = {
                "chrf_mean": mean(all_chrf_scores),
                "chrf_std": stdev(all_chrf_scores) if len(all_chrf_scores) > 1 else 0.0,
                "edit_mean": mean(all_edit_scores),
                "edit_std": stdev(all_edit_scores) if len(all_edit_scores) > 1 else 0.0,
                "num_tests": len(all_chrf_scores),
                "num_languages": len(results_by_language)
            }
        
        return summary_stats

    def print_results(self, all_results):
        print(f"\n{'='*70}")
        print("TRUE SOURCE RESULTS - ALL MODELS")
        print(f"{'='*70}")
        
        # Print results for each model
        for model, results_by_language in all_results.items():
            print(f"\n🤖 MODEL: {model}")
            print("-" * 50)
            
            # Print per-language results for this model
            for lang_name, lang_results in results_by_language.items():
                print(f"\n📍 {lang_name}:")
                
                for condition in ["with_source", "without_source"]:
                    chrf_scores = [r["chrf"] for r in lang_results[condition]]
                    edit_scores = [r["edit"] for r in lang_results[condition]]
                    
                    condition_display = condition.replace('_', ' ').title()
                    print(f"  {condition_display}: chrF+ {mean(chrf_scores):.3f}±{stdev(chrf_scores):.3f}, "
                          f"Edit {mean(edit_scores):.3f}±{stdev(edit_scores):.3f}")
                
                # Show improvement for this language
                with_chrf = [r["chrf"] for r in lang_results["with_source"]]
                without_chrf = [r["chrf"] for r in lang_results["without_source"]]
                improvement = mean(with_chrf) - mean(without_chrf)
                print(f"  → Source effect: {improvement:+.3f} chrF+")
            
            # Print overall aggregated results for this model
            print(f"\n🌍 OVERALL RESULTS FOR {model}:")
            print("-" * 40)
        
            overall_results = {}
            for condition in ["with_source", "without_source"]:
                all_chrf = []
                all_edit = []
                for lang_results in results_by_language.values():
                    all_chrf.extend([r["chrf"] for r in lang_results[condition]])
                    all_edit.extend([r["edit"] for r in lang_results[condition]])
                
                overall_results[condition] = {"chrf": all_chrf, "edit": all_edit}
                condition_display = condition.replace('_', ' ').title()
                print(f"{condition_display}: chrF+ {mean(all_chrf):.3f}±{stdev(all_chrf):.3f}, "
                      f"Edit {mean(all_edit):.3f}±{stdev(all_edit):.3f} "
                      f"({len(all_chrf)} tests across {len(results_by_language)} languages)")
            
            # Show overall source effect for this model
            print(f"\n📈 SOURCE EFFECT FOR {model}:")
            print("-" * 25)
            overall_improvement = mean(overall_results["with_source"]["chrf"]) - mean(overall_results["without_source"]["chrf"])
            print(f"chrF+ improvement with source: {overall_improvement:+.3f}")

    def save_results(self, all_results, detailed_results, output_file):
        # Compute summary statistics for all models
        all_summary_stats = {}
        for model, results_by_language in all_results.items():
            all_summary_stats[model] = self.compute_summary_stats(results_by_language)
        
        # Get languages tested from first model
        first_model_results = next(iter(all_results.values()))
        
        output_data = {
            "benchmark": "true_source",
            "models": self.models,
            "languages_tested": list(first_model_results.keys()),
            "summary": all_summary_stats,
            "detailed_results": detailed_results
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\n💾 Results saved to: {output_file}")


def main():
    load_dotenv()
    
    parser = argparse.ArgumentParser(description="True Source Benchmark")
    # Use project root's Corpus directory
    default_corpus = str(Path(__file__).parent.parent / "Corpus")
    parser.add_argument("--corpus-dir", default=default_corpus)
    parser.add_argument("--source-file", default="eng-engULB.txt")
    parser.add_argument("--model", default="gpt-4o", help="Single model to test")
    parser.add_argument("--models", nargs="+", help="Multiple models to compare")
    parser.add_argument("--num-tests", type=int, default=15)
    parser.add_argument("--output", type=str)
    
    args = parser.parse_args()
    
    # Handle both --model and --models arguments
    models = args.models if args.models else [args.model]
    
    benchmark = TrueSourceBenchmark(args.corpus_dir, args.source_file, models)
    benchmark.run_benchmark(args.num_tests, args.output)
    print("\n✅ True source benchmark completed!")
    return 0


if __name__ == "__main__":
    exit(main()) 